{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHzjriKmcqoWi8CT831/wv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yogeshsinghgit/Pwskills_Assignment/blob/main/Image_Scrapper_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAMKwlhGXkO-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Scrapper Assignment\n",
        "\n",
        "[Assignment Link](https://drive.google.com/file/d/1cHsg7RefjZl9QmKy6d0mPKBIipr2w6ar/view)"
      ],
      "metadata": {
        "id": "Gz45g7ZtXm4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Go to this given URL and solve the following questions\n",
        "URL: https://www.youtube.com/@PW-Foundation/videos"
      ],
      "metadata": {
        "id": "LV8lC1E6XrKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. Write a python program to extract the video URL of the first five videos."
      ],
      "metadata": {
        "id": "Q-R2wKJqX6kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_youtube_channel_video_urls(channel_url, num_videos=5):\n",
        "    # Fetch the HTML content of the channel page\n",
        "    response = requests.get(channel_url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "    # Parse the HTML content with BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract the URLs of the first num_videos videos\n",
        "    video_urls = []\n",
        "    for video in soup.find_all('a', {'id': 'video-title'}):\n",
        "        video_url = video.get('href')\n",
        "        if video_url:\n",
        "            video_urls.append(f\"https://www.youtube.com{video_url}\")\n",
        "            if len(video_urls) == num_videos:\n",
        "                break\n",
        "\n",
        "    return video_urls\n",
        "\n",
        "# Example: Replace 'YOUR_CHANNEL_URL' with the actual URL of the YouTube channel\n",
        "channel_url = 'YOUR_CHANNEL_URL'\n",
        "video_urls = get_youtube_channel_video_urls(channel_url, num_videos=5)\n",
        "\n",
        "# Display the video URLs\n",
        "print(\"URLs of the first 5 videos:\")\n",
        "for i, url in enumerate(video_urls, start=1):\n",
        "    print(f\"{i}. {url}\")\n"
      ],
      "metadata": {
        "id": "0AuZpSh2ZG-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. Write a python program to extract the URL of the video thumbnails of the first five videos."
      ],
      "metadata": {
        "id": "hw3SdS68YYI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_youtube_channel_thumbnails(channel_url, num_videos=5):\n",
        "    # Fetch the HTML content of the channel page\n",
        "    response = requests.get(channel_url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "    # Parse the HTML content with BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract the thumbnails of the first num_videos videos\n",
        "    thumbnails = []\n",
        "    for video in soup.find_all('a', {'id': 'video-title'}):\n",
        "        thumbnail = video.find_previous('img')\n",
        "        if thumbnail:\n",
        "            thumbnail_url = thumbnail.get('src')\n",
        "            if thumbnail_url:\n",
        "                thumbnails.append(thumbnail_url)\n",
        "                if len(thumbnails) == num_videos:\n",
        "                    break\n",
        "\n",
        "    return thumbnails\n",
        "\n",
        "# Example: Replace 'YOUR_CHANNEL_URL' with the actual URL of the YouTube channel\n",
        "channel_url = 'YOUR_CHANNEL_URL'\n",
        "thumbnails = get_youtube_channel_thumbnails(channel_url, num_videos=5)\n",
        "\n",
        "# Download the thumbnails\n",
        "for i, thumbnail_url in enumerate(thumbnails, start=1):\n",
        "    response = requests.get(thumbnail_url)\n",
        "    if response.status_code == 200:\n",
        "        with open(f\"thumbnail_{i}.jpg\", 'wb') as f:\n",
        "            f.write(response.content)\n",
        "            print(f\"Thumbnail {i} downloaded successfully.\")\n",
        "    else:\n",
        "        print(f\"Failed to download thumbnail {i}. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "uEL0MvoIYVnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. Write a python program to extract the title of the first five videos."
      ],
      "metadata": {
        "id": "vSHriaYCY_QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_youtube_channel_titles(channel_url, num_videos=5):\n",
        "    # Fetch the HTML content of the channel page\n",
        "    response = requests.get(channel_url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "    # Parse the HTML content with BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract the titles of the first num_videos videos\n",
        "    video_titles = []\n",
        "    for video in soup.find_all('a', {'id': 'video-title'}):\n",
        "        title = video.get('title')\n",
        "        if title:\n",
        "            video_titles.append(title)\n",
        "            if len(video_titles) == num_videos:\n",
        "                break\n",
        "\n",
        "    return video_titles\n",
        "\n",
        "# Example: Replace 'YOUR_CHANNEL_URL' with the actual URL of the YouTube channel\n",
        "channel_url = 'YOUR_CHANNEL_URL'\n",
        "titles = get_youtube_channel_titles(channel_url, num_videos=5)\n",
        "\n",
        "# Display the titles\n",
        "print(\"Titles of the first 5 videos:\")\n",
        "for i, title in enumerate(titles, start=1):\n",
        "    print(f\"{i}. {title}\")\n"
      ],
      "metadata": {
        "id": "gwf2HcVAYpZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4. Write a python program to extract the number of views of the first five videos."
      ],
      "metadata": {
        "id": "C62tASvzZLSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_youtube_channel_view_counts(channel_url, num_videos=5):\n",
        "    # Fetch the HTML content of the channel page\n",
        "    response = requests.get(channel_url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "    # Parse the HTML content with BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract the view counts of the first num_videos videos\n",
        "    view_counts = []\n",
        "    for count_elem in soup.find_all('span', {'class': 'style-scope ytd-grid-video-renderer'}):\n",
        "        view_count = count_elem.get_text(strip=True)\n",
        "        if view_count:\n",
        "            view_counts.append(view_count)\n",
        "            if len(view_counts) == num_videos:\n",
        "                break\n",
        "\n",
        "    return view_counts\n",
        "\n",
        "# Example: Replace 'YOUR_CHANNEL_URL' with the actual URL of the YouTube channel\n",
        "channel_url = 'YOUR_CHANNEL_URL'\n",
        "view_counts = get_youtube_channel_view_counts(channel_url, num_videos=5)\n",
        "\n",
        "# Display the view counts\n",
        "print(\"Number of views for the first 5 videos:\")\n",
        "for i, count in enumerate(view_counts, start=1):\n",
        "    print(f\"{i}. {count}\")\n"
      ],
      "metadata": {
        "id": "OD2B64tDYzFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5. Write a python program to extract the time of posting of video for the first five videos."
      ],
      "metadata": {
        "id": "xc9G7oDhZUm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_youtube_channel_upload_times(channel_url, num_videos=5):\n",
        "    # Fetch the HTML content of the channel page\n",
        "    response = requests.get(channel_url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "    # Parse the HTML content with BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract the upload times of the first num_videos videos\n",
        "    upload_times = []\n",
        "    for time_elem in soup.find_all('div', {'class': 'style-scope ytd-grid-video-renderer'}):\n",
        "        time_text = time_elem.get_text(strip=True)\n",
        "        if 'ago' in time_text:\n",
        "            upload_times.append(time_text)\n",
        "            if len(upload_times) == num_videos:\n",
        "                break\n",
        "\n",
        "    return upload_times\n",
        "\n",
        "# Example: Replace 'YOUR_CHANNEL_URL' with the actual URL of the YouTube channel\n",
        "channel_url = 'YOUR_CHANNEL_URL'\n",
        "upload_times = get_youtube_channel_upload_times(channel_url, num_videos=5)\n",
        "\n",
        "# Display the upload times\n",
        "print(\"Upload times for the first 5 videos:\")\n",
        "for i, upload_time in enumerate(upload_times, start=1):\n",
        "    print(f\"{i}. {upload_time}\")\n"
      ],
      "metadata": {
        "id": "KKJFDP9JZW3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to create a csv file and store the data into it\n",
        "\n",
        "```python\n",
        "import csv\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    [\"Name\", \"Age\", \"City\"],\n",
        "    [\"John Doe\", 25, \"New York\"],\n",
        "    [\"Jane Smith\", 30, \"Los Angeles\"],\n",
        "    [\"Bob Johnson\", 22, \"Chicago\"]\n",
        "]\n",
        "\n",
        "# Specify the file path\n",
        "csv_file_path = \"sample_data.csv\"\n",
        "\n",
        "# Write data to the CSV file\n",
        "with open(csv_file_path, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    \n",
        "    # Write the data\n",
        "    for row in data:\n",
        "        writer.writerow(row)\n",
        "\n",
        "print(f\"CSV file '{csv_file_path}' created successfully.\")\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "cazMssfPZqKU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0czetsD6ZwJq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}